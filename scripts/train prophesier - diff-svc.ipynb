{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea16415",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_name = \"bestpony\" # I don't know if it's okay to use spaces here\n",
    "dataset_path = \"/workspace/data.zip\"\n",
    "binary_path = \"/workspace/binary.zip\" # use a binary.zip file if you have one, otherwise one will be created here\n",
    "model_path = \"/workspace/models\" # pick a directory for storing .ckpt (model) files\n",
    "mount_gdrive = '' # set this to '/gdrive' if you're on Colab and want to use your gdrive\n",
    "save_every = 2000 # 2000 steps = every ~10m on an RTX 3090, ~30m on RTX 3080\n",
    "\n",
    "install_dependencies = True # set this to false if you've already successfully run this cell\n",
    "install_directory = '/workspace/diff-svc' # if you're running locally, pick a directory you can modify\n",
    "\n",
    "# after setting everything above, run this cell\n",
    "# ---------------------------------------------\n",
    "\n",
    "import os\n",
    "\n",
    "if mount_gdrive:\n",
    "    from google.colab import drive\n",
    "    drive.mount(mount_gdrive, force_remount=True)\n",
    "\n",
    "args_ok = True\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    print(f\"there's nothing in your dataset path ({dataset_path})\")\n",
    "    print(\" -> please update it and run this cell again\")\n",
    "    args_ok = False\n",
    "\n",
    "if args_ok:\n",
    "    if not os.path.exists(os.path.dirname(install_directory)):\n",
    "        os.makedirs(os.path.dirname(install_directory), exist_ok=True)\n",
    "    checkpoint_dir = f\"{install_directory}/checkpoints\"\n",
    "\n",
    "    # install dependencies\n",
    "\n",
    "    if install_dependencies:\n",
    "        # download prereq packages\n",
    "        !git clone https://github.com/synthbot-anon/emotional-diff-svc.git \"{install_directory}\"\n",
    "        !(cd \"{install_directory}\"; git checkout -f cc000a254be89f467de881e6c5c48d7b9b8e590f)\n",
    "        !sudo apt install -y zip libsndfile1 gcc\n",
    "        !pip install torch torchvision torchaudio librosa h5py matplotlib praat-parselmouth pyloudnorm torchcrepe webrtcvad scikit-image pycwt\n",
    "        !pip install -r \"{install_directory}/requirements_short.txt\"\n",
    "        !pip install --upgrade pytorch-lightning\n",
    "\n",
    "        # download prereq model checkpoints\n",
    "        os.mkdir(checkpoint_dir)\n",
    "        os.chdir(checkpoint_dir)\n",
    "        !wget https://github.com/justinjohn0306/diff-svc/releases/download/models/0102_xiaoma_pe.zip\n",
    "        !wget https://github.com/justinjohn0306/diff-svc/releases/download/models/hubert.zip\n",
    "        !wget https://github.com/openvpi/vocoders/releases/download/nsf-hifigan-v1/nsf_hifigan_20221211.zip\n",
    "        !unzip 0102_xiaoma_pe.zip\n",
    "        !unzip hubert.zip\n",
    "        !unzip nsf_hifigan_20221211.zip\n",
    "        !rm *.zip\n",
    "    \n",
    "    if os.path.abspath(model_path) != os.path.abspath(checkpoint_dir):\n",
    "        # leave the checkpoint path alone if that's where we want to store models\n",
    "        \n",
    "        if not os.path.exists(model_path):\n",
    "            os.makedirs(model_path, exist_ok=True)\n",
    "        \n",
    "        character_path = os.path.join(checkpoint_dir, character_name)\n",
    "        !rm -r \"{character_path}\" 2> /dev/null\n",
    "        os.symlink(model_path, character_path)\n",
    "    \n",
    "    if not os.path.exists(os.path.dirname(binary_path)):\n",
    "        os.makedirs(os.path.dirname(binary_path, exist_ok=True))\n",
    "  \n",
    "    print('done')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a4694c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the data... just run this cell\n",
    "\n",
    "unpack_data = True #@param {type:\"boolean\"}\n",
    "create_config = True #@param {type:\"boolean\"}\n",
    "\n",
    "# create the data dir\n",
    "if unpack_data:\n",
    "    print('unpacking the dataset')\n",
    "\n",
    "    data_dir = os.path.join(install_directory, 'data', 'raw', character_name)\n",
    "    !rm -r {data_dir} 2> /dev/null\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    from zipfile import ZipFile\n",
    "    import soundfile\n",
    "    from io import BytesIO\n",
    "    import librosa\n",
    "\n",
    "    # unpack zipped data\n",
    "    modified_data = False\n",
    "    mod_types = set()\n",
    "\n",
    "    with ZipFile(dataset_path, 'r') as data:\n",
    "        for info in data.infolist():\n",
    "            if info.is_dir():\n",
    "                continue\n",
    "\n",
    "            fn = info.filename\n",
    "\n",
    "            # unpack the data\n",
    "            with data.open(fn) as inp:\n",
    "                sound_data = inp.read()            \n",
    "                try:\n",
    "                    audio, sr = soundfile.read(BytesIO(sound_data))\n",
    "                except:\n",
    "                    print('skipping invalid file in {os.path.basename(dataset_path)}:', fn)\n",
    "                    continue\n",
    "\n",
    "                # force the new filename to match the required format\n",
    "                dirname, basename = os.path.split(os.path.normpath(fn))\n",
    "                dir_paths = dirname.split(os.sep)\n",
    "                dir_paths = list(filter(lambda x: x not in ('', 'data', 'raw', character_name), dir_paths))\n",
    "                new_dir = '-'.join(dir_paths)\n",
    "                new_path = os.path.join(new_dir, basename)\n",
    "\n",
    "                # normalize audio if needed\n",
    "                if sr != 44100:\n",
    "                    print('resampling', new_path)\n",
    "                    audio, sr = librosa.resample(audio, sr, 44100)\n",
    "                    modified_data = True\n",
    "                    mod_types.add('resampled audio to 44.1 khz')\n",
    "\n",
    "                # write to disk\n",
    "                write_path = os.path.join(data_dir, new_path)\n",
    "                os.makedirs(os.path.dirname(write_path), exist_ok=True)\n",
    "                soundfile.write(write_path, audio, sr)\n",
    "\n",
    "    if modified_data:\n",
    "        print('the data was modified:')\n",
    "        print('  -', '\\n  - '.join(mod_types))\n",
    "        print(f'saving a copy of the modifed data to {install_directory}/updated-data.zip')\n",
    "        !python3 -m zipfile -c \"{install_directory}/updated-data.zip\" \"{data_dir}\"\n",
    "        print('Hey! Please use ^ in the future.')\n",
    "        print('--------------------------------')\n",
    "        print('')\n",
    "        print('')\n",
    "\n",
    "    print('done unpacking the dataset')\n",
    "\n",
    "if create_config:\n",
    "    print('creating the config file... ', end='')\n",
    "    import yaml\n",
    "    import glob\n",
    "\n",
    "    config_path = os.path.join(install_directory, 'training', 'config_nsf.yaml')\n",
    "    with open(config_path) as inp:\n",
    "        config = yaml.full_load(inp)\n",
    "\n",
    "    config['binary_data_dir'] = f'data/binary/{character_name}'\n",
    "    config['raw_data_dir'] = f'data/raw/{character_name}'\n",
    "    config['speaker_id'] = character_name\n",
    "    config['work_dir'] = f'checkpoints/{character_name}'\n",
    "    config['val_check_interval'] = save_every\n",
    "\n",
    "    with open(config_path, 'w') as outp:\n",
    "        yaml.dump(config, outp)\n",
    "    \n",
    "    model_config = os.path.join(model_path, 'config.yaml')\n",
    "    if glob.glob(f'{model_path}/*.ckpt') or os.path.exists(model_config):\n",
    "        with open(model_config, 'w') as outp:\n",
    "            yaml.dump(config, outp)\n",
    "\n",
    "    print('done')\n",
    "\n",
    "\n",
    "if os.path.exists(binary_path):\n",
    "    print('using preprocessed data from', binary_path)\n",
    "    !python3 -m zipfile -e \"{binary_path}\" \"{install_directory}/data/binary\"\n",
    "    print('done')\n",
    "else:\n",
    "    print('preprocessing the data...')\n",
    "    !rm -r \"{install_directory}/data/binary\" 2> /dev/null\n",
    "    os.chdir(install_directory)\n",
    "    !PYTHONPATH=. CUDA_VISIBLE_DEVICES=0 python3 preprocessing/binarize.py --config training/config_nsf.yaml\n",
    "    print('packing the results... ', end='')\n",
    "    !python3 -m zipfile -c \"{binary_path}\" \"{install_directory}/data/binary\"\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20bb64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(install_directory)\n",
    "if glob.glob(f\"{model_path}/*.ckpt\"):\n",
    "    print('resuming from last checkpoint')\n",
    "    !CUDA_VISIBLE_DEVICES=0 python run.py --config training/config_nsf.yaml --exp_name \"{character_name}\"\n",
    "else:\n",
    "    print('starting a new training run')\n",
    "    !CUDA_VISIBLE_DEVICES=0 python run.py --config training/config_nsf.yaml --exp_name \"{character_name}\" --reset\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
